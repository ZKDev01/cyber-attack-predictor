{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capa 1: Heurísticas tipo ModSecurity (no ML)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ==============================\n",
    "# Cargar dataset base\n",
    "# ==============================\n",
    "# Ajusta el path si es necesario\n",
    "df = pd.read_csv('access_log_structured.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f004d2f",
   "metadata": {},
   "source": [
    "## Investigación y Justificación de la Capa Heurística\n",
    "\n",
    "### 1. Planteamiento del problema específico\n",
    "\n",
    "El tráfico analizado en los registros de acceso presenta una alta proporción de peticiones automatizadas (bots, scrapers y spiders) caracterizadas por comportamientos persistentes y de baja intensidad. En este escenario, los mecanismos clásicos de rate limiting global resultan insuficientes, ya que el volumen total de peticiones no alcanza umbrales críticos a nivel del sistema, mientras que entidades individuales (IPs o User-Agents) exhiben patrones anómalos sostenidos en el tiempo.\n",
    "\n",
    "El problema no consiste en detectar picos de tráfico, sino en identificar **desviaciones relativas de comportamiento por entidad**, lo cual justifica el uso de una capa heurística stateful basada en reglas.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Limitaciones del rate limiting global\n",
    "\n",
    "El rate limiting tradicional aplica umbrales fijos de peticiones por unidad de tiempo a nivel global o por endpoint. Este enfoque presenta las siguientes limitaciones en el contexto de logs reales:\n",
    "\n",
    "- No distingue entre tráfico humano y automatizado.\n",
    "- Es vulnerable a ataques distribuidos de baja intensidad (*low-and-slow*).\n",
    "- No captura patrones persistentes que solo son evidentes al analizar el historial por IP.\n",
    "- Penaliza tráfico legítimo durante picos normales de uso.\n",
    "\n",
    "Desde un punto de vista teórico, el rate limiting global opera sobre una **métrica absoluta**, mientras que el tráfico automatizado moderno se caracteriza por **anomalías relativas**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Rate limiting adaptativo por IP y User-Agent\n",
    "\n",
    "El enfoque propuesto redefine el rate limiting como un problema de **detección de outliers por entidad**, utilizando métricas estadísticas relativas:\n",
    "\n",
    "- Frecuencia de peticiones por IP\n",
    "- Proporción respecto a la media del conjunto\n",
    "- Persistencia temporal\n",
    "- Diversidad de recursos accedidos\n",
    "\n",
    "Formalmente, una IP se considera anómala si:\n",
    "\n",
    "\n",
    "\n",
    "requests(IP) > k × mean(requests(all IPs)), k > 1\n",
    "\n",
    "\n",
    "Este criterio permite detectar *heavy hitters*, una técnica ampliamente utilizada en detección de anomalías de red y mitigación de ataques distribuidos.\n",
    "\n",
    "**Ventaja teórica clave**:  \n",
    "Este método es invariante al volumen total de tráfico y se adapta automáticamente a la carga normal del sistema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d9bae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 4. Análisis comportamental vs inspección puntual\n",
    "\n",
    "A diferencia de la inspección por petición individual, la capa heurística se basa en **agregación y memoria de estado**, lo cual permite:\n",
    "\n",
    "- Identificar patrones que no son maliciosos de forma aislada.\n",
    "- Penalizar comportamientos repetitivos y persistentes.\n",
    "- Diferenciar entre errores ocasionales y exploración automatizada.\n",
    "\n",
    "Esto convierte el sistema en un **detector de comportamiento**, no en un filtro sintáctico.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Uso del User-Agent como factor de score\n",
    "\n",
    "El User-Agent no se emplea como una regla determinista, sino como una señal parcial dentro de un sistema de scoring:\n",
    "\n",
    "- User-Agents asociados a herramientas automatizadas aumentan el score.\n",
    "- User-Agents vacíos o genéricos se penalizan.\n",
    "- La decisión final depende de la combinación con métricas de frecuencia y error.\n",
    "\n",
    "Desde un punto de vista teórico, este enfoque reduce falsos positivos al evitar decisiones basadas en una única característica fácilmente falsificable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_agent = pd.DataFrame(df['user_agent'].value_counts())\n",
    "display(df_user_agent.head(5))\n",
    "display(df_user_agent.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bot(user_agent:str) -> bool:\n",
    "  \"Detecta si un user-agent pertenece a un bot/crawler/spider\"\n",
    "  if not user_agent:\n",
    "    return False \n",
    "  user_agent = user_agent.lower()\n",
    "  \n",
    "  # Nombres de bots conocidos\n",
    "  known_bots = {\n",
    "    'googlebot', \n",
    "    'bingbot', \n",
    "    'yandexbot', \n",
    "    'applebot',\n",
    "    'duckduckbot', \n",
    "    'baiduspider', \n",
    "    'sogou', \n",
    "    'bytespider',\n",
    "    'amazonbot', \n",
    "    'gptbot', \n",
    "    'chatgpt-user', \n",
    "    'oai-searchbot',\n",
    "    'claudebot', \n",
    "    'google-cloudvertexbot', \n",
    "    'google-extended',\n",
    "    'perplexitybot', \n",
    "    'meta-externalagent', \n",
    "    'meta-webindexer',\n",
    "    'tiktokspider', \n",
    "    'openai.com-bot', \n",
    "    'google.bot',\n",
    "    # Poco comunes pero encontrados en el archivo de access log\n",
    "    'thinkbot', \n",
    "    'petalbot'\n",
    "    # No es bot pero se asumirá que sí debido a que los comportamientos no son permitidos\n",
    "    'securitytxtresearch'\n",
    "    #'SecurityTxtResearch'\n",
    "  }\n",
    "\n",
    "  # Verificar nombres de bots conocidos\n",
    "  for bot in known_bots:\n",
    "    if bot in user_agent:\n",
    "      return True\n",
    "\n",
    "  # Patrón: \"dominio.com-bot\" o \"dominio.bot\"\n",
    "  import re\n",
    "  pattern = r'[a-z0-9.-]+\\.(?:com|org|net|io)[-.]bot'\n",
    "  if re.search(pattern, user_agent):\n",
    "    return True\n",
    "\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_bot'] = df['user_agent'].apply(is_bot)\n",
    "\n",
    "display(df.head())\n",
    "display(df['is_bot'].value_counts())\n",
    "\n",
    "\n",
    "# Crear máscara para los bots\n",
    "bot_mask = df['is_bot'] == True\n",
    "# Modificar la columna 'attack' para los bots \n",
    "df['anomaly_score_ua'] = bot_mask.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754e2c5",
   "metadata": {},
   "source": [
    "### Contar requests por IP en ventanas cortas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = '30s'      # ventana corta\n",
    "REQ_THRESHOLD = 20 # requests por ventana\n",
    "\n",
    "ip_rate = (\n",
    "    df\n",
    "    .set_index('timestamp')\n",
    "    .groupby('ip_client')\n",
    "    .resample(WINDOW)\n",
    "    .size()\n",
    "    .reset_index(name='req_count')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d4e41",
   "metadata": {},
   "source": [
    "### detectar ventanas anómalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0259a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_rate['is_high_rate_window'] = ip_rate['req_count'] > REQ_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a730e",
   "metadata": {},
   "source": [
    "### agregar a nivel IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c031fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_rate_stats = ip_rate.groupby('ip_client').agg(\n",
    "    high_rate_windows=('is_high_rate_window', 'sum'),\n",
    "    max_requests_window=('req_count', 'max')\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c7713",
   "metadata": {},
   "source": [
    "### score heurístico por IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb700ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ip_rate_stats['rate_anomaly'] = ip_rate_stats['high_rate_windows'] >= 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e19bbd",
   "metadata": {},
   "source": [
    "### Ahora fusionamos frecuencia total + error rate + rate temporal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ip_stats = df.groupby('ip_client').agg(\n",
    "    total_requests=('ip_client', 'count'),\n",
    "    unique_urls=('request', 'nunique'),\n",
    "    error_rate=('status', lambda x: np.mean(x.astype(str).str.startswith(('4','5'))))\n",
    ").reset_index()\n",
    "\n",
    "ip_stats = ip_stats.merge(ip_rate_stats, on='ip_client', how='left')\n",
    "\n",
    "ip_stats[['high_rate_windows','max_requests_window']] = (\n",
    "    ip_stats[['high_rate_windows','max_requests_window']]\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "mean_requests = ip_stats['total_requests'].mean()\n",
    "ip_stats['high_rate'] = ip_stats['total_requests'] > 5 * mean_requests\n",
    "\n",
    "ip_stats['anomaly_score_ip'] = (\n",
    "    (ip_stats['high_rate_windows'] >= 2).astype(int) +\n",
    "    (ip_stats['error_rate'] > 0.4).astype(int) +\n",
    "    ip_stats[['high_rate']].sum(axis=1)\n",
    ")\n",
    "\n",
    "ip_stats['is_banned'] = ip_stats['anomaly_score_ip'] >= 2\n",
    "\n",
    "df = df.merge(ip_stats[['ip_client','anomaly_score_ip','is_banned']], on='ip_client', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79be77",
   "metadata": {},
   "source": [
    "\n",
    "### Clasificación heurística final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(row):\n",
    "    score = 0\n",
    "    score += row.get('anomaly_score_ip',0)\n",
    "    score += row.get('anomaly_score_ua',0)\n",
    "    return 'Anomaly' if score >= 2 else 'Normal'\n",
    "\n",
    "df['heuristic_label'] = df.apply(classify, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19a946",
   "metadata": {},
   "source": [
    "\n",
    "### Sistemas stateful y reputación dinámica\n",
    "\n",
    "El mantenimiento de estado por IP y User-Agent permite construir una reputación dinámica basada en el historial de comportamiento. Este enfoque es equivalente a un **sistema de reglas stateful**, donde las decisiones se basan en acumulación de evidencia.\n",
    "\n",
    "Este modelo es especialmente eficaz contra:\n",
    "- Scraping persistente\n",
    "- Reconocimiento progresivo\n",
    "- Ataques automatizados que evitan umbrales fijos\n",
    "\n",
    "Teóricamente, la incorporación de memoria transforma reglas locales en un sistema de decisión globalmente consistente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf56d5",
   "metadata": {},
   "source": [
    "### Limpieza de columnas y resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b77d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_clean = df.drop(columns=['ip_client','user_agent','ident','referrer','is_bot'], errors='ignore')\n",
    "\n",
    "print(df_clean['heuristic_label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ef11f",
   "metadata": {},
   "source": [
    "\n",
    "###  Relación con ataques reales\n",
    "\n",
    "Las heurísticas implementadas permiten detectar:\n",
    "\n",
    "- Scraping: alta frecuencia + diversidad de URLs\n",
    "- Bots: patrones de User-Agent + ritmo constante\n",
    "- Reconocimiento: alta tasa de errores HTTP\n",
    "- DoS lento: persistencia por IP\n",
    "\n",
    "Estas detecciones no requieren firmas explícitas, sino desviaciones de comportamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49a0ed6",
   "metadata": {},
   "source": [
    "\n",
    "### Respaldo teórico y referencias\n",
    "\n",
    "- NIST SP 800-94 – Intrusion Detection and Prevention Systems  \n",
    "  https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-94.pdf\n",
    "\n",
    "- Sommer & Paxson – *Outside the Closed World*  \n",
    "  https://ieeexplore.ieee.org/document/5504793\n",
    "\n",
    "- ModSecurity Reference Manual  \n",
    "  https://github.com/SpiderLabs/ModSecurity/wiki/Reference-Manual-(v2.x)\n",
    "\n",
    "- Cloudflare – Adaptive Rate Limiting  \n",
    "  https://www.cloudflare.com/ddos/\n",
    "\n",
    "- Estan & Varghese – Heavy-Hitter Detection  \n",
    "  https://dl.acm.org/doi/10.1145/316188.316214\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
