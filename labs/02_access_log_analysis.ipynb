{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Callable\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "# Librerías para el procesamiento de access logs \n",
    "import re \n",
    "from parse import parse \n",
    "from lars.apache import ApacheSource, COMBINED, ApacheWarning\n",
    "\n",
    "# Manejo de advertencias del sistema, usada para capturar las líneas que no pueden parsearse por problemas de lars (ApacheWarning)\n",
    "import warnings\n",
    "\n",
    "# Configuración de estilo para las gráficas\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Ejecutar antes las celdas de `01_access_log_parsing` o elige una ubicación donde exista un archivo de access logs (Apache Combined Log Format) \n",
    "log_file_path = '../data/target/access_log_master.log'\n",
    "log_prep_path = '../data/target/access_log_master.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c98c0",
   "metadata": {},
   "source": [
    "### Expresión General para Apache Combined Log Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ec60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATTERN = re.compile(\n",
    "  r'(?P<ip_client>\\S+)\\s+'      # IP del cliente \n",
    "  r'(?P<ident>\\S+)\\s+'          # ident \n",
    "  r'(?P<auth_user>\\S+)\\s+'      # auth_user\n",
    "  r'\\[(?P<timestamp>.+?)\\]\\s+'  # timestamp\n",
    "  r'\\\"(?P<request>.*?)\\\"\\s+'    # línea completa de request: método + url + protocolo\n",
    "  r'(?P<status>\\d{3})\\s+'       # código de estado\n",
    "  r'(?P<size>\\S+)\\s+'           # tamaño de respuesta (bytes)\n",
    "  r'\\\"(?P<referer>.*?)\\\"\\s+'    # referer\n",
    "  r'\\\"(?P<user_agent>.*?)\\\"'    # user-agent\n",
    ")\n",
    "\n",
    "def parse_apache_logs(log_file_path, add_n_line:bool=False, del_request:bool=True):\n",
    "  \"Parsea logs de Apache extrayendo todos los campos solicitados\"\n",
    "  parsed_logs = []\n",
    "  failed_lines = []\n",
    "  \n",
    "  with open(log_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line_num, line in enumerate(file, 1):\n",
    "      line = line.strip()\n",
    "      if not line:\n",
    "        continue\n",
    "      \n",
    "      match_ = LOG_PATTERN.match(line)\n",
    "      \n",
    "      if match_:\n",
    "        # Obtener todos los campos capturados\n",
    "        log_data = match_.groupdict()\n",
    "        \n",
    "        # Separar request en método, URL y protocolo\n",
    "        request_parts = log_data['request'].split()\n",
    "        match len(request_parts):\n",
    "          case 3:\n",
    "            log_data['method'] = request_parts[0]\n",
    "            log_data['url'] = request_parts[1]\n",
    "            log_data['protocol'] = request_parts[2]\n",
    "          case 2:\n",
    "            log_data['method'] = request_parts[0]\n",
    "            log_data['url'] = request_parts[1]\n",
    "            log_data['protocol'] = 'UNKNOWN'\n",
    "          case 1:\n",
    "            log_data['method'] = 'UNKNOWN'\n",
    "            log_data['url'] = request_parts[0]\n",
    "            log_data['protocol'] = 'UNKNOWN'\n",
    "\n",
    "        if add_n_line:\n",
    "          # Añadir número de línea\n",
    "          log_data['line_number'] = line_num\n",
    "        \n",
    "        if del_request:\n",
    "          del log_data['request']\n",
    "        \n",
    "        parsed_logs.append(log_data)\n",
    "      else:\n",
    "        # Guardar línea fallida\n",
    "        failed_lines.append({\n",
    "          'line': line_num,\n",
    "          'content': line[:100]\n",
    "        })\n",
    "  \n",
    "  return parsed_logs, failed_lines\n",
    "\n",
    "logs, errors = parse_apache_logs(log_file_path)\n",
    "\n",
    "print(f\"Cantidad de logs parseados: {len(logs)}\")\n",
    "print(f\"Cantidad de logs fallidos:  {len(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspección de logs parseados\n",
    "logs[len(logs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5ded6",
   "metadata": {},
   "source": [
    "Crear una columna para etiquetar manualmente si un log es un ataque web o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(logs)\n",
    "\n",
    "# Columna para representar si un log es un ataque: -1 si no se conoce, 1 si es ataque, o 0 si no es un ataque \n",
    "df['anomaly'] = -1\n",
    "\n",
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformar 'timestamp' a datetime (ajustando el formato de Apache) \n",
    "def parse_timestamps_vectorized(df):\n",
    "  df['timestamp_clean'] = df['timestamp'].astype(str).str.strip('[]')\n",
    "  \n",
    "  df['timestamp_parsed'] = pd.to_datetime(\n",
    "    df['timestamp_clean'],\n",
    "    format='%d/%b/%Y:%H:%M:%S %z', \n",
    "    errors='coerce'\n",
    "  )\n",
    "  \n",
    "  mask = df['timestamp_parsed'].isna()\n",
    "  if mask.any():\n",
    "    df.loc[mask, 'timestamp_parsed'] = pd.to_datetime(\n",
    "      df.loc[mask, 'timestamp_clean'],\n",
    "      format='%d/%b/%Y:%H:%M:%S',\n",
    "      errors='coerce'\n",
    "    )\n",
    "\n",
    "  df['timestamp'] = df['timestamp_parsed']\n",
    "  df = df.drop(['timestamp_parsed', 'timestamp_clean'], axis=1)\n",
    "  \n",
    "  return df \n",
    "\n",
    "df = parse_timestamps_vectorized(df)\n",
    "# transformar 'size' a numérico (manejando valores no numéricos como '-')\n",
    "df['size'] = pd.to_numeric(df['size'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fad00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rango de Tiempo: {df['timestamp'].min()} -> {df['timestamp'].max()}\")\n",
    "print(f\"Estadísticas de 'size': Min={ df['size'].min() }, Max={ df['size'].max() }, Mean={ df['size'].mean() }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las primeras K filas con los valores 'size' más altos\n",
    "K = 10\n",
    "display_cols = [\n",
    "  \n",
    "]\n",
    "# Ordenar por tamaño de forma descendente y tomar las primeras K filas\n",
    "top_K_largest = df.nlargest(K, 'size')\n",
    "above_mean = df[df['size'] > df['size'].mean()]\n",
    "print(f\"Logs con tamaño de respuesta > promedio (promedio/mean={df['size'].mean():.2f}): {len(above_mean)} ({(len(above_mean)/len(df))*100:.2f}%)\")\n",
    "display(top_K_largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2102f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Crear scatter plot con transparencia para manejar muchos puntos\n",
    "plt.scatter(df['timestamp'], df['size'], alpha=0.6, s=10, color='royalblue')\n",
    "\n",
    "plt.title('Tamaño de respuestas a lo largo del tiempo', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Fecha y Hora', fontsize=12)\n",
    "plt.ylabel('Tamaño (bytes)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# agrupar por día \n",
    "df_daily = df.set_index('timestamp').resample('D').size()\n",
    "# crear gráfica de barras para peticiones diarias \n",
    "bars = plt.bar(df_daily.index, df_daily.values, width=0.8, color='skyblue', edgecolor='navy', alpha=0.8)\n",
    "\n",
    "plt.title('Número de peticiones por día', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Fecha', fontsize=12)\n",
    "plt.ylabel('Cantidad de peticiones', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "  height = bar.get_height()\n",
    "  plt.text(bar.get_x() + bar.get_width()/2, height, f'{int(height)}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b62e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = [\n",
    "  'status',\n",
    "  'method',\n",
    "  'user_agent'\n",
    "]\n",
    "\n",
    "# Filtrar solo columnas existentes\n",
    "existing_cols = [col for col in important_columns if col in df.columns]\n",
    "n_cols = len(existing_cols)\n",
    "\n",
    "# Configurar subplots\n",
    "fig, axes = plt.subplots(n_cols, 1, figsize=(15, 5 * n_cols))\n",
    "if n_cols == 1:\n",
    "  axes = [axes]\n",
    "\n",
    "# Crear gráficas para cada columna\n",
    "for idx, col in enumerate(existing_cols):\n",
    "  ax = axes[idx]\n",
    "  \n",
    "  # Obtener value counts (top 10 para evitar sobrecarga)\n",
    "  value_counts = df[col].value_counts().head(10)\n",
    "  \n",
    "  # Para columnas con muchos valores únicos, mostrar solo top\n",
    "  if len(value_counts) > 10:\n",
    "    others_count = df[col].value_counts().iloc[10:].sum()\n",
    "    if others_count > 0:\n",
    "      value_counts['Otros'] = others_count\n",
    "  \n",
    "  # Crear gráfica de barras horizontal\n",
    "  bars = ax.barh(range(len(value_counts)), value_counts.values, color=plt.cm.tab20c(range(len(value_counts))))\n",
    "  \n",
    "  ax.set_title(f'Distribución de {col}', fontsize=12, fontweight='bold')\n",
    "  ax.set_xlabel('Frecuencia', fontsize=10)\n",
    "  ax.set_yticks(range(len(value_counts)))\n",
    "  ax.set_yticklabels(value_counts.index, fontsize=9)\n",
    "  \n",
    "  # Añadir etiquetas de valores\n",
    "  total = value_counts.sum()\n",
    "  for i, v in enumerate(value_counts.values):\n",
    "    percentage = (v / total) * 100\n",
    "    ax.text(v + max(value_counts.values) * 0.01, i, f'{v} ({percentage:.1f}%)', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15118ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica adicional: Códigos de estado HTTP (si existe la columna)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Agrupar códigos por categoría\n",
    "df['status_category'] = df['status'].astype(str).str[0] + '00'\n",
    "\n",
    "# Contar por categoría\n",
    "status_counts = df['status_category'].value_counts()\n",
    "\n",
    "# Crear gráfico de pastel\n",
    "colors = ['#4CAF50', '#2196F3', '#FF9800', '#F44336', '#9C27B0']\n",
    "wedges, texts, autotexts = plt.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "\n",
    "# Mejorar etiquetas\n",
    "for autotext in autotexts:\n",
    "  autotext.set_color('white')\n",
    "  autotext.set_fontweight('bold')\n",
    "\n",
    "plt.title('Distribución de códigos de estado HTTP por categoría', fontsize=14, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "\n",
    "# Añadir leyenda con códigos específicos más comunes\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_status = df['status'].value_counts().head(10)\n",
    "top_status.plot(kind='barh', color='steelblue')\n",
    "plt.title('Top 10 códigos de estado HTTP más frecuentes', fontsize=12)\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b1d7a",
   "metadata": {},
   "source": [
    "### Búsqueda Automática de Logs Maliciosos a través del Parsing\n",
    "\n",
    "Se puede utilizar la biblioteca de `lars` para identificar logs maliciosos, para ello se analizar todo el archivo de logs para intentar parsear cada línea y se separan los logs en 3 categorías: \n",
    "- `parsed_logs`: logs que fueron parseados con éxito y se extrajeron todas las características deseadas de esos logs (estas características las extrae `lars` pero no se tendrán en cuenta en la primera parte del análisis y parsing de logs)\n",
    "- `conflicting_logs`: logs que fueron parseados con éxito pero existen características que no pueden ser extraídas (en el código siguiente se intenta manejar de forma automática estos tipos de logs y se definen valores `None` en caso de fallar al pedir una característica específica del log parseado por `lars`)\n",
    "- `problematic_logs`: logs que no fueron parseados por `lars` debido a que presentan un patrón diferente al definido por `lars`, que es un patrón estándar para los logs de Apache (`ApacheSource`) con formato de logs combinado (`log_format=COMBINED` incluye User-Agent y Referrer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7376d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '{ip_client} {ident} {auth_user} [{timestamp}] {request_http} {status:d} {size:d} {referrer} {agent}'\n",
    "\n",
    "parsed_logs = []\n",
    "conflicting_logs = {}\n",
    "problematic_logs = {}\n",
    "\n",
    "with open(log_file_path, 'r') as f:\n",
    "  all_lines = f.readlines()\n",
    "\n",
    "# Lista de campos de URL que se quieren extraer de la clase Row de lars\n",
    "url_fields = [\n",
    "  ('request_url_scheme', 'scheme'),\n",
    "  ('request_url_netloc', 'netloc'),\n",
    "  ('request_url_path_str', 'path_str'),\n",
    "  ('request_url_params', 'params'),\n",
    "  ('request_url_query_str', 'query_str'),\n",
    "  ('request_url_fragment', 'fragment')\n",
    "]\n",
    "\n",
    "# Función para extraer campos de URL \n",
    "def extract_url_fields(url_obj):\n",
    "  result = {}\n",
    "  for field_name, attr_name in url_fields:\n",
    "    try:\n",
    "      value = getattr(url_obj, attr_name, None)\n",
    "      result[field_name] = value if value else None\n",
    "    except Exception:\n",
    "      result[field_name] = None\n",
    "  return result\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "  warnings.simplefilter(\"always\", ApacheWarning)\n",
    "\n",
    "  with open(log_file_path) as f:\n",
    "    with ApacheSource(f, log_format=COMBINED) as source:\n",
    "      for i, row in enumerate(source, 1):\n",
    "        try:\n",
    "          record = {\n",
    "            \"remote_host\": row.remote_host,\n",
    "            \"ident\": row.ident,\n",
    "            \"remote_user\": row.remote_user,\n",
    "            \"time\": row.time,\n",
    "            \"request_method\": row.request.method if hasattr(row, 'request') and row.request else None,\n",
    "            \"request_protocol\": row.request.protocol if hasattr(row, 'request') and row.request else None,\n",
    "            \"status\": row.status,\n",
    "            \"size\": row.size,\n",
    "            \"req_Referer\": row.req_Referer,\n",
    "            \"req_User_agent\": row.req_User_agent,\n",
    "          }\n",
    "          \n",
    "          # Extraer campos de URL si existe\n",
    "          if hasattr(row, 'request') and row.request and row.request.url:\n",
    "            url_data = extract_url_fields(row.request.url)\n",
    "            record.update(url_data)\n",
    "          else:\n",
    "            # Si no hay URL, establecer todos los campos como None\n",
    "            record.update({field_name: None for field_name, _ in url_fields})\n",
    "          \n",
    "          line_content = all_lines[i-1].strip()\n",
    "          parsed = parse(regex, line_content)\n",
    "          if parsed:\n",
    "            record['request_http'] = parsed.named['request_http']\n",
    "          else: \n",
    "            print(f\"Error?: Line: {line_content}\")\n",
    "          \n",
    "          parsed_logs.append(record)\n",
    "        except Exception as e:\n",
    "          conflicting_logs[i] = {\n",
    "            'error': str(e),\n",
    "            'line_content': all_lines[i-1].strip() if i <= len(all_lines) else \"not available\"\n",
    "          }\n",
    "  \n",
    "  for warning in w:\n",
    "    if issubclass(warning.category, ApacheWarning):\n",
    "      msg = str(warning.message)\n",
    "      match = re.search(r'Line (\\d+):', msg)\n",
    "      if match:\n",
    "        line_num = int(match.group(1))\n",
    "        if line_num <= len(all_lines):\n",
    "          problematic_logs[line_num] = {\n",
    "            'line_content': all_lines[line_num - 1].strip(),\n",
    "            'warning_message': msg,\n",
    "            'category': warning.category.__name__,\n",
    "          }\n",
    "\n",
    "total_problematic_logs = len(problematic_logs)\n",
    "total_conflicting_logs = len(conflicting_logs)\n",
    "total_parsed_logs = len(parsed_logs)\n",
    "total_logs = total_parsed_logs + total_problematic_logs + total_conflicting_logs\n",
    "\n",
    "print(f\"Total de logs procesados: {total_logs}\")\n",
    "print(f\"Problematic Logs: {total_problematic_logs} ({(total_problematic_logs / total_logs * 100 if total_logs != 0 else 0):.2f}%)\")\n",
    "print(f\"Conflicting Logs: {total_conflicting_logs} ({(total_conflicting_logs / total_logs * 100 if total_logs != 0 else 0):.2f}%)\")\n",
    "print(f\"Parsed Logs: {total_parsed_logs} ({(total_parsed_logs / total_logs * 100 if total_logs != 0 else 0):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idx = list(problematic_logs.keys())\n",
    "max_idx = max(all_idx)\n",
    "\n",
    "line_problematics = [value['line_content'] for _,value in problematic_logs.items()]\n",
    "\n",
    "for idx,log in problematic_logs.items():\n",
    "  print(f\"{idx:>{len(str(max_idx))}}) {log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspección de logs parseados\n",
    "i = len(all_idx) - 1\n",
    "display(logs[all_idx[i]-1])\n",
    "display(df.iloc[all_idx[i]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792be493",
   "metadata": {},
   "source": [
    "Presentan:\n",
    "- Cadenas bytes en formato hexademical en las solicitudes HTTP (tanto en método, protocolo y url)\n",
    "- No existe Referer, ni User-Agent\n",
    "- Las pocas solicitudes HTTP (las que no están formadas por bytes en formato hexademical) no presentan método y tienen cadenas muy extrañas o tienen direcciones IP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import parse \n",
    "\n",
    "pattern = '{ip_client} {ident} {auth_user} [{timestamp}] {request_http} {status:d} {size:d} \"{referrer}\" \"{agent}\"'\n",
    "\n",
    "problematic_parsed_logs = []\n",
    "for problematic in list(problematic_logs.items()):\n",
    "  idx = problematic[0]\n",
    "  line_content = problematic[1]['line_content']\n",
    "  try: \n",
    "    parsed = parse(pattern, line_content.strip())\n",
    "    problematic_parsed_logs.append(parsed.named)\n",
    "  except Exception as e:\n",
    "    print(f\"Line {idx}: {line_content}\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "df_problematic = pd.DataFrame(problematic_parsed_logs)\n",
    "status_distribution = df_problematic['status'].value_counts().sort_index()\n",
    "display(df_problematic.head(3))\n",
    "display(df_problematic.shape[0])\n",
    "display(status_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acaf41",
   "metadata": {},
   "source": [
    "**Análisis de Distribución de Código de Estado HTTP**:\n",
    "- `301`: El recurso solicitado tiene una nueva URL permanente \n",
    "- `400`: El servidor no puede entender la solicitud debido a una sintaxis inválida \n",
    "- `408`: El servidor esperó demasiado tiempo a que el cliente enviara la solicitud completa y cerró la conexión inactiva\n",
    "- `499`: El cliente cerro la solicitud antes de que el servidor pudiera responder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298bbc2",
   "metadata": {},
   "source": [
    "Para códigos 400, se va revisar la dirección IP de origen y el agente de usuario $\\to$ Un alto volumen de errores 400 provenientes de unas pocas IPs o de agentes de usuario inusuales (como herramientas de escaneo) puede indicar actividad maliciosa automatizada. \n",
    "\n",
    "Para códigos 408 y 499: Estos códigos suelen ser síntomas de que algo en el servidor no funciona de manera óptima $\\to$ Un aumento repentino suele estar relacionado con una alta carga en el servidor, tiempos de respuesta lentos en la aplicación o problemas de red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar logs con los códigos de estado de interés\n",
    "status_codes_to_check = [400, 408, 499]\n",
    "filtered_logs = df_problematic[df_problematic['status'].isin(status_codes_to_check)]\n",
    "\n",
    "# Agrupar por IP para identificar posibles orígenes problemáticos\n",
    "ip_analysis = filtered_logs.groupby(['status', 'ip_client']).size().reset_index(name='count')\n",
    "print(ip_analysis.sort_values(by='count', ascending=False).head(20))  # Ver las 20 IPs más activas\n",
    "\n",
    "# Agrupar por agente de usuario\n",
    "agent_analysis = filtered_logs.groupby(['status', 'agent']).size().reset_index(name='count')\n",
    "print(agent_analysis.sort_values(by='count', ascending=False).head(10))  # Ver los 10 agentes más comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b40809",
   "metadata": {},
   "source": [
    "Estos resultados muestran una **actividad maliciosa**: patrón de cientos de errores 400, concentrados en pocas IPs, cada una con rangos muy diferentes, y con agente vacío, no es un comportamiento normal de un usuario o buscador. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13385108",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_idx = [idx-1 for idx in all_idx] \n",
    "df.loc[adjusted_idx, 'anomaly'] = 1\n",
    "display(df['anomaly'].head())\n",
    "display(df[ df['anomaly'] == 1 ].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b748e6",
   "metadata": {},
   "source": [
    "### Análisis de `user-agent` sospechosos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_agent = pd.DataFrame(df['user_agent'].value_counts())\n",
    "display(df_user_agent.head(5))\n",
    "display(df_user_agent.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97626db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bot(user_agent:str) -> bool:\n",
    "  \"Detecta si un user-agent pertenece a un bot/crawler/spider\"\n",
    "  if not user_agent:\n",
    "    return False \n",
    "  user_agent = user_agent.lower()\n",
    "  \n",
    "  # Nombres de bots conocidos\n",
    "  known_bots = {\n",
    "    'googlebot', \n",
    "    'bingbot', \n",
    "    'yandexbot', \n",
    "    'applebot',\n",
    "    'duckduckbot', \n",
    "    'baiduspider', \n",
    "    'sogou', \n",
    "    'bytespider',\n",
    "    'amazonbot', \n",
    "    'gptbot', \n",
    "    'chatgpt-user', \n",
    "    'oai-searchbot',\n",
    "    'claudebot', \n",
    "    'google-cloudvertexbot', \n",
    "    'google-extended',\n",
    "    'perplexitybot', \n",
    "    'meta-externalagent', \n",
    "    'meta-webindexer',\n",
    "    'tiktokspider', \n",
    "    'openai.com-bot', \n",
    "    'google.bot',\n",
    "    # Poco comunes pero encontrados en el archivo de access log\n",
    "    'thinkbot', \n",
    "    'petalbot'\n",
    "    # No es bot pero se asumirá que sí debido a que los comportamientos no son permitidos\n",
    "    'securitytxtresearch'\n",
    "    #'SecurityTxtResearch'\n",
    "  }\n",
    "\n",
    "  # Verificar nombres de bots conocidos\n",
    "  for bot in known_bots:\n",
    "    if bot in user_agent:\n",
    "      return True\n",
    "\n",
    "  # Patrón: \"dominio.com-bot\" o \"dominio.bot\"\n",
    "  import re\n",
    "  pattern = r'[a-z0-9.-]+\\.(?:com|org|net|io)[-.]bot'\n",
    "  if re.search(pattern, user_agent):\n",
    "    return True\n",
    "\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_bot'] = df['user_agent'].apply(is_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df['is_bot'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3512fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear máscara para los bots\n",
    "bot_mask = df['is_bot'] == True\n",
    "# Modificar la columna 'attack' para los bots \n",
    "df.loc[bot_mask, 'anomaly'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f4ec0",
   "metadata": {},
   "source": [
    "### Análisis de Métodos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f004b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[(df['method'] != 'GET') & (df['method'] != 'POST')]\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_counts = df_filtered['method'].value_counts()\n",
    "percentage_series = (method_counts / len(df_filtered) * 100).round(2)\n",
    "summary_df = pd.DataFrame({\n",
    "  'count': method_counts, \n",
    "  'percentage': percentage_series\n",
    "})\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del_method = df_filtered[df_filtered['method'] == 'DELETE']\n",
    "display(df_del_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf229cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trace_method = df_filtered[df_filtered['method'] == 'TRACE']\n",
    "display(df_trace_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_method_mask = df['method'] == 'DELETE'\n",
    "df.loc[del_method_mask, 'anomaly'] = 1\n",
    "trace_method_mask = df['method'] == 'TRACE'\n",
    "df.loc[trace_method_mask, 'anomaly'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a894af9",
   "metadata": {},
   "source": [
    "### Análisis de IPs de Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e377bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['ip_client'] == '::1']\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['url'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['ip_client'] == '::1'\n",
    "df.loc[mask, 'anomaly'] = 0      # El resultado de investigar los logs con estos IP llego a ser que son logs no maliciosos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38019cb0",
   "metadata": {},
   "source": [
    "### Extracción y Codificación de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sql_words(url):\n",
    "  \"Cuenta palabras relacionadas con SQL Injection\" \n",
    "  sql_words = [\n",
    "    r'SELECT', \n",
    "    r'FROM', \n",
    "    r'WHERE', \n",
    "    r'DELETE', \n",
    "    r'DROP', \n",
    "    r'CREATE', \n",
    "    r'TABLE', \n",
    "    r'LIKE', \n",
    "    r'UNION', \n",
    "    r'INSERT', \n",
    "    r'UPDATE', \n",
    "    r'ALTER',\n",
    "    r'INTO', \n",
    "    r'VALUES', \n",
    "    r'SET', \n",
    "    r'JOIN', \n",
    "    r'GRANT', \n",
    "    r'REVOKE'\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(sql_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_xss_words(url):\n",
    "  \"Cuenta palabras relacionadas con Cross-Site Scripting (XSS)\"\n",
    "  xss_words = [\n",
    "    r'script', \n",
    "    r'alert', \n",
    "    r'javascript', \n",
    "    r'onerror', \n",
    "    r'onload', \n",
    "    r'onunload', \n",
    "    r'prompt', \n",
    "    r'confirm', \n",
    "    r'eval', \n",
    "    r'expression',\n",
    "    r'function\\(', \n",
    "    r'xmlhttprequest', \n",
    "    r'xhr', \n",
    "    r'window\\.', \n",
    "    r'document\\.', \n",
    "    r'iframe', \n",
    "    r'src=', \n",
    "    r'cookie', \n",
    "    r'document\\.cookie',\n",
    "    r'set-cookie', \n",
    "    r'click', \n",
    "    r'mouseover'\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(xss_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_command_words(url): \n",
    "  \"Cuenta palabras relacionadas con ejecución de comandos\"\n",
    "  command_words = [\n",
    "    r'cmd', \n",
    "    r'dir', \n",
    "    r'shell', \n",
    "    r'exec', \n",
    "    r'cat', \n",
    "    r'etc', \n",
    "    r'tmp',\n",
    "    r'bin', \n",
    "    r'bash', \n",
    "    r'sh', \n",
    "    r'python', \n",
    "    r'perl', \n",
    "    r'ruby', \n",
    "    r'php',\n",
    "    r'\\.exe', \n",
    "    r'\\.php', \n",
    "    r'\\.js', \n",
    "    r'\\.py', \n",
    "    r'\\.pl', \n",
    "    r'\\.rb',\n",
    "    r'system\\(', \n",
    "    r'popen\\(', \n",
    "    r'proc_open\\(', \n",
    "    r'passthru\\('\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(command_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_auth_words(url):\n",
    "  \"Cuenta palabras relacionadas con autentificación\"\n",
    "  auth_words = [\n",
    "    r'admin', \n",
    "    r'administrator', \n",
    "    r'password', \n",
    "    r'login', \n",
    "    r'pwd',\n",
    "    r'credential', \n",
    "    r'user', \n",
    "    r'username', \n",
    "    r'passwd', \n",
    "    r'secret',\n",
    "    r'token', \n",
    "    r'session', \n",
    "    r'auth', \n",
    "    r'authentication', \n",
    "    r'key'\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(auth_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_error_words(url):\n",
    "  \"Cuenta palabras relacionadas con errores\"\n",
    "  e_words = [\n",
    "    r'error', \n",
    "    r'errorMsg', \n",
    "    r'errorID', \n",
    "    r'incorrect', \n",
    "    r'fail',\n",
    "    r'failed', \n",
    "    r'failure', \n",
    "    r'exception', \n",
    "    r'stack',\n",
    "    r'trace',\n",
    "    r'debug', \n",
    "    r'warning', \n",
    "    r'fatal', \n",
    "    r'crash',\n",
    "    r'invalid'\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(e_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_malware_words(url):\n",
    "  \"Cuenta palabras relacionadas con malware\"\n",
    "  malware_words = [\n",
    "    r'malware', \n",
    "    r'ransomware', \n",
    "    r'phishing', \n",
    "    r'exploit', \n",
    "    r'virus',\n",
    "    r'trojan', \n",
    "    r'backdoor', \n",
    "    r'spyware', \n",
    "    r'rootkit', \n",
    "    r'worm',\n",
    "    r'adware', \n",
    "    r'keylogger', \n",
    "    r'botnet', \n",
    "    r'payload', \n",
    "    r'inject',\n",
    "    r'injected', \n",
    "    r'hacker', \n",
    "    r'attack', \n",
    "    r'exploit', \n",
    "    r'breach'\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(malware_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_danger_characters(url):\n",
    "  \"Cuenta caracteres potencialmente peligrosos\"\n",
    "  characters = [\n",
    "    r\"'\", \n",
    "    r\"--\", \n",
    "    r\";\", \n",
    "    r\"\\\\\", \n",
    "    r\"\\\"\", \n",
    "    r\"<\", \n",
    "    r\">\", \n",
    "    r\"(\", \n",
    "    r\")\", \n",
    "    r\"&\", \n",
    "    r\"|\"\n",
    "  ]\n",
    "  count = 0\n",
    "  url_str = str(url)\n",
    "  for c in characters:\n",
    "    count += url_str.count(c)\n",
    "  return count\n",
    "\n",
    "def count_obfuscation_code_words(url):\n",
    "  \"Cuenta técnicas de ofuscación de código\"\n",
    "  obfuscation_words = [\n",
    "    r'encode', \n",
    "    r'decode', \n",
    "    r'base64', \n",
    "    r'hex', \n",
    "    r'urlencode',\n",
    "    r'urldecode', \n",
    "    r'escape', \n",
    "    r'unescape', \n",
    "    r'obfuscate',\n",
    "    r'xor', \n",
    "    r'rot13', \n",
    "    r'chr\\(',\n",
    "    r'char\\(', \n",
    "    r'fromCharCode',\n",
    "    r'eval\\('\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(obfuscation_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_dir_words(url):\n",
    "  \"Cuenta referencias a directorios sensibles\"\n",
    "  dir_words = [\n",
    "    r'\\.\\./', \n",
    "    r'\\.\\.\\\\', \n",
    "    r'/etc/', \n",
    "    r'/bin/', \n",
    "    r'/tmp/', \n",
    "    r'/var/',\n",
    "    r'/home/', \n",
    "    r'/root/', \n",
    "    r'proc/', \n",
    "    r'dev/', \n",
    "    r'boot/', \n",
    "    r'usr/', \n",
    "    r'lib/', \n",
    "    r'sbin/'\n",
    "  ]\n",
    "  pattern = re.compile('|'.join(dir_words), re.IGNORECASE)\n",
    "  matches = pattern.findall(str(url))\n",
    "  return len(matches)\n",
    "\n",
    "def count_dot(url):\n",
    "  \"Cuenta la cantidad de puntos en la URL\"\n",
    "  url = str(url)\n",
    "  return url.count('.')\n",
    "\n",
    "def count_http(url): \n",
    "  \"Cuenta las ocurrencias de http en la URL\"\n",
    "  url = str(url)\n",
    "  return url.count('http')\n",
    "\n",
    "def count_percentage_symbol(url):\n",
    "  \"Cuenta los signos de porcentaje\"\n",
    "  url = str(url)\n",
    "  return url.count('%')\n",
    "\n",
    "def count_question_symbol(url):\n",
    "  \"Cuenta los signos de interrogación\"\n",
    "  url = str(url)\n",
    "  return url.count('?')\n",
    "\n",
    "def count_hyphen(url):\n",
    "  \"Cuenta guiones (-) en la URL\"\n",
    "  url = str(url)\n",
    "  return url.count('-')\n",
    "\n",
    "def count_equal(url):\n",
    "  \"Cuenta signos igual (=)\"\n",
    "  url = str(url)\n",
    "  return url.count('=')\n",
    "\n",
    "def url_length(url):\n",
    "  \"Retorna la longitud total de la URL\"\n",
    "  return len(str(url))\n",
    "\n",
    "def digit_count(url):\n",
    "  \"Cuenta la cantidad de dígitos numéricos en la URL\"\n",
    "  digits = 0\n",
    "  for i in url:\n",
    "    if i.isnumeric():\n",
    "      digits = digits + 1\n",
    "  return digits\n",
    "\n",
    "def letter_count(url):\n",
    "  \"Cuenta la cantidad de letras en la URL\"\n",
    "  letters = 0\n",
    "  for i in url:\n",
    "    if i.isalpha():\n",
    "      letters += 1\n",
    "  return letters\n",
    "\n",
    "def count_special_characters(url):\n",
    "  \"Cuenta caracteres especiales (no alfanuméricos) usando regex\"\n",
    "  special_characters = re.sub(r'[a-zA-Z0-9\\s]', '', url)\n",
    "  count = len(special_characters)\n",
    "  return count\n",
    "\n",
    "def is_encoded(url):\n",
    "  \"\"\"Detecta si la URL está codificada (presencia de %)\n",
    "\n",
    "  Returns:\n",
    "    int: Retorna 1 si es verdadero (URL está codificada) y 0 si no\n",
    "  \"\"\"\n",
    "  return int('%' in url.lower())\n",
    "\n",
    "def unusual_character_ratio(url):\n",
    "  \"Calcula la proporción de caracteres inusuales (no alfanuméricos, guiones, puntos o guiones bajos) respecto a la longitud total\"\n",
    "  total_characters = len(url)\n",
    "  unusual_characters = re.sub(r'[a-zA-Z0-9\\s\\-._]', '', url)\n",
    "  unusual_count = len(unusual_characters)\n",
    "  ratio = unusual_count / total_characters if total_characters > 0 else 0\n",
    "  return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features_functions: List[Callable] = [\n",
    "  count_sql_words,\n",
    "  count_xss_words, \n",
    "  count_command_words, \n",
    "  count_auth_words, \n",
    "  count_error_words,\n",
    "  count_malware_words,\n",
    "  count_danger_characters,\n",
    "  count_obfuscation_code_words,\n",
    "  count_dir_words,\n",
    "  count_dot,\n",
    "  count_http,\n",
    "  count_percentage_symbol,\n",
    "  count_question_symbol,\n",
    "  count_hyphen,\n",
    "  count_equal,\n",
    "  url_length,\n",
    "  digit_count,\n",
    "  letter_count,\n",
    "  count_special_characters,\n",
    "  is_encoded,\n",
    "  unusual_character_ratio\n",
    "]\n",
    "\n",
    "def safe_str_conversion(value):\n",
    "  if pd.isna(value):\n",
    "    return ''\n",
    "  if isinstance(value, str):\n",
    "    return value \n",
    "  # para cualquier otro tipo (int, float, etc), convertir a str \n",
    "  return str(value)\n",
    "df['url'] = df['url'].apply(safe_str_conversion)\n",
    "\n",
    "request_columns_name = []\n",
    "for func in extract_features_functions:\n",
    "  feat_name = f\"url__{func.__name__}\"\n",
    "  df[feat_name] = df['url'].apply(func)\n",
    "  request_columns_name.append(feat_name)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codificar ip-client\n",
    "import ipaddress\n",
    "\n",
    "def encode_ip(ip):\n",
    "  if pd.isna(ip):\n",
    "    return 2  \n",
    "  ip_str = str(ip).strip()\n",
    "  \n",
    "  if ip_str == '::1':\n",
    "    return 1  # localhost\n",
    "  \n",
    "  return 0    # other\n",
    "\n",
    "df['encode_ip'] = df['ip_client'].apply(encode_ip)\n",
    "display(df['encode_ip'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3679142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_log(df:pd.DataFrame, col_name):\n",
    "  if col_name not in df.columns:\n",
    "    raise Exception(\"La columna no existe en el DataFrame\")\n",
    "  serie = df[col_name].copy()\n",
    "  serie = serie.astype(str).str.strip()\n",
    "  encoder_result = np.where(\n",
    "    serie.isin(['-', '', 'nan', 'None', 'null']) | (serie == 'nan'),\n",
    "    0,  # ausente/no aplicable\n",
    "    1   # presente\n",
    "  )\n",
    "  return encoder_result\n",
    "\n",
    "df['ident_encoded'] = encoder_log(df, 'ident')\n",
    "df['user_encoded']  = encoder_log(df, 'auth_user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cab594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_method(method):\n",
    "  \"Verifica si el método parece un método HTTP válido\"\n",
    "  if pd.isna(method):\n",
    "    return False\n",
    "  # Métodos HTTP válidos según RFC + 'UNKNOWN'\n",
    "  valid_pattern = r'^[A-Z_-]{1,20}$'\n",
    "  return bool(re.match(valid_pattern, str(method)))\n",
    "\n",
    "# Identificar valores inválidos\n",
    "invalid_mask = ~df['method'].apply(is_valid_method)\n",
    "invalid_methods = df.loc[invalid_mask, 'method'].unique()\n",
    "\n",
    "print(f\"Valores inválidos/ruido encontrados: {len(invalid_methods)}\")\n",
    "if len(invalid_methods) > 0:\n",
    "  print(\"Ejemplos:\", invalid_methods[:5])\n",
    "\n",
    "# Marcar como 'INVALID'\n",
    "df['method'] = df['method'].apply(lambda x: 'INVALID' if not is_valid_method(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85715728",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_methods = ['DELETE', 'TRACE'] # 'PUT', 'REQMOD'\n",
    "\n",
    "# Filtrar el DataFrame para obtener solo estas solicitudes\n",
    "df_target = df[df['method'].isin(target_methods)].copy()\n",
    "\n",
    "# Ordenar por método para mejor visualización\n",
    "df_target = df_target.sort_values('method')\n",
    "\n",
    "# Mostrar todas las filas con columnas relevantes\n",
    "cols_to_display = ['method', 'url', 'ip_client', 'user_agent']\n",
    "# Ajustar según las columnas disponibles en tu DataFrame\n",
    "available_cols = [col for col in cols_to_display if col in df_target.columns]\n",
    "\n",
    "print(df_target[available_cols].to_string(index=False))\n",
    "\n",
    "# Si hay muchas columnas, podemos mostrar un resumen compacto\n",
    "for method in target_methods:\n",
    "  method_data = df_target[df_target['method'] == method]\n",
    "  if len(method_data) > 0:\n",
    "    print(f\"\\n--- {method} ({len(method_data)} solicitudes) ---\")\n",
    "    # Mostrar URL únicas y IPs asociadas\n",
    "    print(f\"URLs únicas: {method_data['url'].nunique()}\")\n",
    "    for url in method_data['url'].unique():\n",
    "      ips = method_data[method_data['url'] == url]['ip_client'].unique()\n",
    "      print(f\"  - {url[:80]}... (IPs: {', '.join(ips)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b722de",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_methods = ['DELETE', 'PUT', 'TRACE', 'REQMOD', 'INVALID']\n",
    "\n",
    "# Crear máscara para identificar las filas con estos métodos\n",
    "mask = df['method'].isin(target_methods)\n",
    "# Establecer attack = 1 para los métodos objetivo\n",
    "df.loc[mask, 'anomaly'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing._label import LabelEncoder\n",
    "\n",
    "status_series = df['status_category'].copy()\n",
    "status_series = status_series.fillna('UNKNOWN')\n",
    "status_encoder = LabelEncoder()\n",
    "df['status_category_encoded'] = status_encoder.fit_transform(status_series)\n",
    "\n",
    "category_mapping = dict(zip(status_encoder.classes_, status_encoder.transform(status_encoder.classes_)))\n",
    "display(category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60080b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_series = df['method'].copy()\n",
    "method_series = method_series.fillna('UNKNOWN')\n",
    "method_encoder = LabelEncoder()\n",
    "df['method_encoded'] = method_encoder.fit_transform(method_series)\n",
    "\n",
    "status_mapping = dict(zip(method_encoder.classes_, method_encoder.transform(method_encoder.classes_)))\n",
    "display(status_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d57bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_column = 'anomaly'\n",
    "columns_selected = [\n",
    "  'size', \n",
    "  'method_encoded', \n",
    "  'is_bot', \n",
    "  'encode_ip', \n",
    "  'status_category_encoded', \n",
    "  'ident_encoded',\n",
    "  'user_encoded'\n",
    "] + request_columns_name + [y_label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df[columns_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfcc41",
   "metadata": {},
   "source": [
    "### Análisis y Etiquetado Manual de Datos Restantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ecfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(df.head(5))\n",
    "display(df['anomaly'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = df[df['anomaly'] != -1]\n",
    "unlabeled_data = df[df['anomaly'] == -1]\n",
    "display(unlabeled_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f783fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def manual_labeling(unlabeled_data:pd.DataFrame, columns_selected:List[str], _max:int=20):\n",
    "  for idx in unlabeled_data.index[:_max]:\n",
    "    clear_output(wait=True) \n",
    "    data = unlabeled_data.loc[idx] \n",
    "    \n",
    "    # Mostrar información del log no etiquetado\n",
    "    max_len_columns_name = max([len(col_name) for col_name in columns_selected])\n",
    "    for col_name in columns_selected:\n",
    "      print(f\"{col_name:>{max_len_columns_name}}: {data[col_name]}\")\n",
    "    \n",
    "    option = input(\"\\n0=Normal, 1=Anomalía, s=Saltar, q=Salir: \").strip()\n",
    "\n",
    "    match option:\n",
    "      case '0':\n",
    "        df.at[idx, 'anomaly'] = 0\n",
    "      case '1':\n",
    "        df.at[idx, 'anomaly'] = 1 \n",
    "      case 'q':\n",
    "        break \n",
    "\n",
    "  return unlabeled_data\n",
    "\n",
    "df_manual = manual_labeling(\n",
    "  unlabeled_data, \n",
    "  columns_selected=[\n",
    "    'ip_client', \n",
    "    'ident',\n",
    "    'auth_user',\n",
    "    'status', \n",
    "    'size', \n",
    "    'user_agent', \n",
    "    'method', \n",
    "    'url', \n",
    "    'protocol',\n",
    "  ], \n",
    "  _max=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_manual.head(10))\n",
    "idx_update = df_manual[df_manual['anomaly'] != -1].index\n",
    "df.loc[idx_update, 'anomaly'] = df_manual.loc[idx_update, 'anomaly']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771c9ac",
   "metadata": {},
   "source": [
    "### Guardar el Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(log_prep_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
