{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "# Librerías para el procesamiento de access logs \n",
    "import re \n",
    "from parse import parse \n",
    "from lars.apache import ApacheSource, COMBINED, ApacheWarning\n",
    "\n",
    "# Manejo de advertencias del sistema, usada para capturar las líneas que no pueden parsearse por problemas de lars (ApacheWarning)\n",
    "import warnings\n",
    "\n",
    "# Configuración de estilo para las gráficas\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Ejecutar antes las celdas de `01_access_log_parsing` o elige una ubicación donde exista un archivo de access logs (Apache Combined Log Format) \n",
    "log_file_path = '../data/target/access_log_master.log'\n",
    "log_prep_path = '../data/target/access_log_master.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56432b",
   "metadata": {},
   "source": [
    "## Load Access Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambia a None para cargar todos los datos, o a un número para establecer un límite\n",
    "n_samples = None \n",
    "\n",
    "with open(log_file_path, 'r') as f:\n",
    "  if n_samples is not None: \n",
    "    sample_lines = []\n",
    "    for _ in range(n_samples):\n",
    "      line = f.readline()\n",
    "      \n",
    "      # si se acaba el archivo antes de alcanzar el número de muestras definidas\n",
    "      if not line:    \n",
    "        break \n",
    "      sample_lines.append(line.strip())\n",
    "  else: \n",
    "    # Cargar todas las líneas\n",
    "    sample_lines = [line.strip() for line in f]\n",
    "\n",
    "print(f\"Sample log lines (Total: {len(sample_lines)}):\")\n",
    "for i,line in enumerate(sample_lines[:10], 1):\n",
    "  print(f\"{i:>{len(str(n_samples))}}) {line}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf69ba9",
   "metadata": {},
   "source": [
    "## Load and Parsing a Access Log File using `lars` y Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '{ip_client} {ident} {auth_user} [{timestamp}] {request_http} {status:d} {size:d} {referrer} {agent}'\n",
    "\n",
    "parsed_logs = []\n",
    "conflicting_logs = {}\n",
    "problematic_logs = {}\n",
    "\n",
    "with open(log_file_path, 'r') as f:\n",
    "  all_lines = f.readlines()\n",
    "\n",
    "# Lista de campos de URL que se quieren extraer de la clase Row de lars\n",
    "url_fields = [\n",
    "  ('request_url_scheme', 'scheme'),\n",
    "  ('request_url_netloc', 'netloc'),\n",
    "  ('request_url_path_str', 'path_str'),\n",
    "  ('request_url_params', 'params'),\n",
    "  ('request_url_query_str', 'query_str'),\n",
    "  ('request_url_fragment', 'fragment')\n",
    "]\n",
    "\n",
    "# Función para extraer campos de URL \n",
    "def extract_url_fields(url_obj):\n",
    "  result = {}\n",
    "  for field_name, attr_name in url_fields:\n",
    "    try:\n",
    "      value = getattr(url_obj, attr_name, None)\n",
    "      result[field_name] = value if value else None\n",
    "    except Exception:\n",
    "      result[field_name] = None\n",
    "  return result\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "  warnings.simplefilter(\"always\", ApacheWarning)\n",
    "\n",
    "  with open(log_file_path) as f:\n",
    "    with ApacheSource(f, log_format=COMBINED) as source:\n",
    "      for i, row in enumerate(source, 1):\n",
    "        try:\n",
    "          record = {\n",
    "            \"remote_host\": row.remote_host,\n",
    "            \"ident\": row.ident,\n",
    "            \"remote_user\": row.remote_user,\n",
    "            \"time\": row.time,\n",
    "            \"request_method\": row.request.method if hasattr(row, 'request') and row.request else None,\n",
    "            \"request_protocol\": row.request.protocol if hasattr(row, 'request') and row.request else None,\n",
    "            \"status\": row.status,\n",
    "            \"size\": row.size,\n",
    "            \"req_Referer\": row.req_Referer,\n",
    "            \"req_User_agent\": row.req_User_agent,\n",
    "          }\n",
    "          \n",
    "          # Extraer campos de URL si existe\n",
    "          if hasattr(row, 'request') and row.request and row.request.url:\n",
    "            url_data = extract_url_fields(row.request.url)\n",
    "            record.update(url_data)\n",
    "          else:\n",
    "            # Si no hay URL, establecer todos los campos como None\n",
    "            record.update({field_name: None for field_name, _ in url_fields})\n",
    "          \n",
    "          line_content = all_lines[i-1].strip()\n",
    "          parsed = parse(regex, line_content)\n",
    "          if parsed:\n",
    "            record['request_http'] = parsed.named['request_http']\n",
    "          else: \n",
    "            print(f\"Error?: Line: {line_content}\")\n",
    "          \n",
    "          parsed_logs.append(record)\n",
    "        except Exception as e:\n",
    "          conflicting_logs[i] = {\n",
    "            'error': str(e),\n",
    "            'line_content': all_lines[i-1].strip() if i <= len(all_lines) else \"not available\"\n",
    "          }\n",
    "  \n",
    "  for warning in w:\n",
    "    if issubclass(warning.category, ApacheWarning):\n",
    "      msg = str(warning.message)\n",
    "      match = re.search(r'Line (\\d+):', msg)\n",
    "      if match:\n",
    "        line_num = int(match.group(1))\n",
    "        if line_num <= len(all_lines):\n",
    "          problematic_logs[line_num] = {\n",
    "            'line_content': all_lines[line_num - 1].strip(),\n",
    "            'warning_message': msg,\n",
    "            'category': warning.category.__name__,\n",
    "          }\n",
    "\n",
    "total_problematic_logs = len(problematic_logs)\n",
    "total_conflicting_logs = len(conflicting_logs)\n",
    "total_parsed_logs = len(parsed_logs)\n",
    "total_logs = total_parsed_logs + total_problematic_logs + total_conflicting_logs\n",
    "\n",
    "print(f\"Total de logs procesados: {total_logs}\")\n",
    "print(f\"Problematic Logs: {total_problematic_logs} ({(total_problematic_logs / total_logs * 100 if total_logs != 0 else 0):.2f}%)\")\n",
    "print(f\"Conflicting Logs: {total_conflicting_logs} ({(total_conflicting_logs / total_logs * 100 if total_logs != 0 else 0):.2f}%)\")\n",
    "print(f\"Parsed Logs: {total_parsed_logs} ({(total_parsed_logs / total_logs * 100 if total_logs != 0 else 0):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87496d26",
   "metadata": {},
   "source": [
    "Mientras que `conflicting_logs` son errores técnicos del parser, `problematic_logs` son anomalías en el contenido que pueden indicar actividad maliciosa o intentos de exploración, pero para confirmar que estos logs representan una actividad maliciosa se debe analizar el contenido de estos logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ad506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = pd.DataFrame(parsed_logs)  \n",
    "display(df_parsed.head(5))\n",
    "display(df_parsed.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051888b",
   "metadata": {},
   "source": [
    "### Análisis de Logs Problemáticos (`problematic_logs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = max(problematic_logs.keys())\n",
    "\n",
    "logs = list(problematic_logs.items()) \n",
    "\n",
    "for log in logs:\n",
    "  idx = log[0]\n",
    "  line_content = log[1]['line_content']\n",
    "  print(f\"{idx:>{len(str(max_idx))}}) {line_content}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a27637",
   "metadata": {},
   "source": [
    "Presentan:\n",
    "- Cadenas bytes en formato hexademical en las solicitudes HTTP (tanto en método, protocolo y url)\n",
    "- No existe Referer, ni User-Agent\n",
    "- Las pocas solicitudes HTTP (las que no están formadas por bytes en formato hexademical) no presentan método y tienen cadenas muy extrañas o tienen direcciones IP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import parse \n",
    "\n",
    "pattern = '{ip_client} {ident} {auth_user} [{timestamp}] {request_http} {status:d} {size:d} \"{referrer}\" \"{agent}\"'\n",
    "\n",
    "problematic_parsed_logs = []\n",
    "for problematic in list(problematic_logs.items()):\n",
    "  idx = problematic[0]\n",
    "  line_content = problematic[1]['line_content']\n",
    "  try: \n",
    "    parsed = parse(pattern, line_content.strip())\n",
    "    problematic_parsed_logs.append(parsed.named)\n",
    "  except Exception as e:\n",
    "    print(f\"Line {idx}: {line_content}\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "df_problematic = pd.DataFrame(problematic_parsed_logs)\n",
    "status_distribution = df_problematic['status'].value_counts().sort_index()\n",
    "display(df_problematic.shape[0])\n",
    "display(status_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891d9d1",
   "metadata": {},
   "source": [
    "**Análisis de Distribución de Código de Estado HTTP**:\n",
    "- `301`: El recurso solicitado tiene una nueva URL permanente \n",
    "- `400`: El servidor no puede entender la solicitud debido a una sintaxis inválida \n",
    "- `408`: El servidor esperó demasiado tiempo a que el cliente enviara la solicitud completa y cerró la conexión inactiva\n",
    "- `499`: El cliente cerro la solicitud antes de que el servidor pudiera responder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdf722",
   "metadata": {},
   "source": [
    "Para códigos 400, se va revisar la dirección IP de origen y el agente de usuario $\\to$ Un alto volumen de errores 400 provenientes de unas pocas IPs o de agentes de usuario inusuales (como herramientas de escaneo) puede indicar actividad maliciosa automatizada. \n",
    "\n",
    "Para códigos 408 y 499: Estos códigos suelen ser síntomas de que algo en el servidor no funciona de manera óptima $\\to$ Un aumento repentino suele estar relacionado con una alta carga en el servidor, tiempos de respuesta lentos en la aplicación o problemas de red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar logs con los códigos de estado de interés\n",
    "status_codes_to_check = [400, 408, 499]\n",
    "filtered_logs = df_problematic[df_problematic['status'].isin(status_codes_to_check)]\n",
    "\n",
    "# Agrupar por IP para identificar posibles orígenes problemáticos\n",
    "ip_analysis = filtered_logs.groupby(['status', 'ip_client']).size().reset_index(name='count')\n",
    "print(ip_analysis.sort_values(by='count', ascending=False).head(20))  # Ver las 20 IPs más activas\n",
    "\n",
    "# Agrupar por agente de usuario\n",
    "agent_analysis = filtered_logs.groupby(['status', 'agent']).size().reset_index(name='count')\n",
    "print(agent_analysis.sort_values(by='count', ascending=False).head(10))  # Ver los 10 agentes más comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3862a",
   "metadata": {},
   "source": [
    "Estos resultados muestran una **actividad maliciosa**: patrón de cientos de errores 400, concentrados en pocas IPs, cada una con rangos muy diferentes, y con agente vacío, no es un comportamiento normal de un usuario o buscador. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c687d5c",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio de Datos sobre Access Log Master File\n",
    "\n",
    "Ya se tienen dos DataFrames (`df_parsed` que son los resultados de parsear correctamente los logs y `df_problematic` son los access logs parseados que presentan un contenido inusual) y se va a analizar ambos como un único DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_parsed.head(3))\n",
    "display(df_problematic.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5defc1df",
   "metadata": {},
   "source": [
    "La relaciones entre las columnas de los DataFrames `df_parsed` y `df_problematic` son:\n",
    "\n",
    "| DataFrame `df_parsed`   | DataFrame `df_problematic` |\n",
    "| ----------------------- | -------------------------- |\n",
    "| `remote_host`           | `ip_client`                |\n",
    "| `ident`                 | `ident`                    |\n",
    "| `remote_user`           | `auth_user`                |\n",
    "| `time`                  | `timestamp`                |\n",
    "| `request_method`        | -                          |\n",
    "| `request_protocol`      | -                          |\n",
    "| `status`                | `status`                   |\n",
    "| `size`                  | `size`                     |\n",
    "| `req_Referer`           | `referrer`                 |\n",
    "| `req_User_agent`        | `agent`                    |\n",
    "| `request_url_scheme`    | -                          |\n",
    "| `request_url_netloc`    | -                          |\n",
    "| `request_url_path_str`  | -                          |\n",
    "| `request_url_params`    | -                          |\n",
    "| `request_url_query_str` | -                          |\n",
    "| `request_url_fragment`  | -                          |\n",
    "| `request_http`          | `request_http`             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7514f",
   "metadata": {},
   "source": [
    "Debido a que el DataFrame `df_problematic` muestra una actividad maliciosa pero no se garantiza nada en el DataFrame de `df_parsed`, no se puede asumir que todos los logs de `df_parsed` presentan una actividad maliciosa, por lo que se va a añadir una columna que sea: `malicious_activity_by_parsing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fafc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir columna \"malicious_activity_by_parsing\" a df_parsed y df_problematic \n",
    "df_problematic['malicious_activity_by_parsing'] = True \n",
    "df_parsed['malicious_activity_by_parsing'] = False \n",
    "\n",
    "# Renombrar columnas de df_problematic para que coincidan con df_parsed (según el mapeo proporcionado)\n",
    "df_problematic = df_problematic.rename(columns={\n",
    "  'ip_client': 'remote_host',\n",
    "  'auth_user': 'remote_user',\n",
    "  'timestamp': 'time',\n",
    "  'referrer': 'req_Referer',\n",
    "  'agent': 'req_User_agent',\n",
    "  # status, size, ident ya tienen el mismo nombre\n",
    "})\n",
    "\n",
    "# Añadir columnas faltantes en df_problematic (que existen en df_parsed pero no en df_problematic)\n",
    "# Estas son las columnas de URL y las relaciones con request\n",
    "url_columns = [\n",
    "  'request_url_scheme',\n",
    "  'request_url_netloc', \n",
    "  'request_url_path_str',\n",
    "  'request_url_params',\n",
    "  'request_url_query_str',\n",
    "  'request_url_fragment'\n",
    "] \n",
    "request_columns = [\n",
    "  'request_method', \n",
    "  'request_protocol'\n",
    "]\n",
    "\n",
    "# Añadir todas las columnas faltantes con valor None\n",
    "for col in url_columns + request_columns:\n",
    "  if col not in df_problematic.columns:\n",
    "    df_problematic[col] = None \n",
    "\n",
    "# Verificar que ambos DataFrames tienen las mismas columnas \n",
    "all_columns = set(df_parsed.columns) | set(df_problematic.columns)\n",
    "\n",
    "# Unificar ambos DataFrames\n",
    "df = pd.concat([df_parsed, df_problematic], ignore_index=True)\n",
    "display(df.head(5))\n",
    "display(df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88adbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformar 'time' a datetime (ajustando el formato de Apache) \n",
    "def parse_apache_time(time_str):\n",
    "  \"Convierte el formato de tiempo de Apache Access Log a datetime\"\n",
    "  try:\n",
    "    # Ejemplo de formato: [01/01/2001:00:00:01 -0700]\n",
    "    return pd.to_datetime(time_str, format='[%d/%b/%Y:%H:%M:%S %z]')\n",
    "  except: \n",
    "    return pd.NaT \n",
    "\n",
    "df['time'] = df['time'].apply(parse_apache_time)\n",
    "\n",
    "# transformar 'size' a numérico (manejando valores no numéricos como '-')\n",
    "df['size'] = pd.to_numeric(df['size'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rango de Tiempo: {df['time'].min()} -> {df['time'].max()}\")\n",
    "print(f\"Estadísticas de 'size': Min={ df['size'].min() }, Max={ df['size'].max() }, Mean={ df['size'].mean() }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1925940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las primeras K filas con los valores 'size' más altos\n",
    "K = 10\n",
    "display_cols = [\n",
    "  'time', \n",
    "  'remote_host', \n",
    "  'request_method', \n",
    "  'request_http', \n",
    "  'size', \n",
    "  'status', \n",
    "  'req_User_agent'\n",
    "]\n",
    "# Ordenar por tamaño de forma descendente y tomar las primeras K filas\n",
    "top_K_largest = df.nlargest(K, 'size')\n",
    "above_mean = df[df['size'] > df['size'].mean()]\n",
    "print(f\"Logs con tamaño de respuesta > promedio (promedio/mean={df['size'].mean():.2f}): {len(above_mean)} ({(len(above_mean)/len(df))*100:.2f}%)\")\n",
    "display(top_K_largest[display_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Crear scatter plot con transparencia para manejar muchos puntos\n",
    "plt.scatter(df['time'], df['size'], alpha=0.6, s=10, color='royalblue')\n",
    "\n",
    "plt.title('Tamaño de respuestas a lo largo del tiempo', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Fecha y Hora', fontsize=12)\n",
    "plt.ylabel('Tamaño (bytes)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e30fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# agrupar por día \n",
    "df_daily = df.set_index('time').resample('D').size()\n",
    "# crear gráfica de barras para peticiones diarias \n",
    "bars = plt.bar(df_daily.index, df_daily.values, width=0.8, color='skyblue', edgecolor='navy', alpha=0.8)\n",
    "\n",
    "plt.title('Número de peticiones por día', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Fecha', fontsize=12)\n",
    "plt.ylabel('Cantidad de peticiones', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "  height = bar.get_height()\n",
    "  plt.text(bar.get_x() + bar.get_width()/2, height, f'{int(height)}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = [\n",
    "  'request_method',\n",
    "  'status',\n",
    "  'remote_host',\n",
    "  'request_http',\n",
    "  'req_User_agent',\n",
    "]\n",
    "\n",
    "# Filtrar solo columnas existentes\n",
    "existing_cols = [col for col in important_columns if col in df.columns]\n",
    "n_cols = len(existing_cols)\n",
    "\n",
    "# Configurar subplots\n",
    "fig, axes = plt.subplots(n_cols, 1, figsize=(15, 5 * n_cols))\n",
    "if n_cols == 1:\n",
    "  axes = [axes]\n",
    "\n",
    "# Crear gráficas para cada columna\n",
    "for idx, col in enumerate(existing_cols):\n",
    "  ax = axes[idx]\n",
    "  \n",
    "  # Obtener value counts (top 10 para evitar sobrecarga)\n",
    "  value_counts = df[col].value_counts().head(10)\n",
    "  \n",
    "  # Para columnas con muchos valores únicos, mostrar solo top\n",
    "  if len(value_counts) > 10:\n",
    "    others_count = df[col].value_counts().iloc[10:].sum()\n",
    "    if others_count > 0:\n",
    "      value_counts['Otros'] = others_count\n",
    "  \n",
    "  # Crear gráfica de barras horizontal\n",
    "  bars = ax.barh(range(len(value_counts)), value_counts.values, color=plt.cm.tab20c(range(len(value_counts))))\n",
    "  \n",
    "  ax.set_title(f'Distribución de {col}', fontsize=12, fontweight='bold')\n",
    "  ax.set_xlabel('Frecuencia', fontsize=10)\n",
    "  ax.set_yticks(range(len(value_counts)))\n",
    "  ax.set_yticklabels(value_counts.index, fontsize=9)\n",
    "  \n",
    "  # Añadir etiquetas de valores\n",
    "  total = value_counts.sum()\n",
    "  for i, v in enumerate(value_counts.values):\n",
    "    percentage = (v / total) * 100\n",
    "    ax.text(v + max(value_counts.values) * 0.01, i, f'{v} ({percentage:.1f}%)', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica adicional: Códigos de estado HTTP (si existe la columna)\n",
    "if 'status' in df.columns:\n",
    "  plt.figure(figsize=(12, 6))\n",
    "    \n",
    "  # Agrupar códigos por categoría\n",
    "  df['status_category'] = df['status'].astype(str).str[0] + '00'\n",
    "    \n",
    "  # Contar por categoría\n",
    "  status_counts = df['status_category'].value_counts()\n",
    "    \n",
    "  # Crear gráfico de pastel\n",
    "  colors = ['#4CAF50', '#2196F3', '#FF9800', '#F44336', '#9C27B0']\n",
    "  wedges, texts, autotexts = plt.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "  \n",
    "  # Mejorar etiquetas\n",
    "  for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "  \n",
    "  plt.title('Distribución de códigos de estado HTTP por categoría', fontsize=14, fontweight='bold')\n",
    "  plt.axis('equal')\n",
    "  \n",
    "  # Añadir leyenda con códigos específicos más comunes\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  top_status = df['status'].value_counts().head(10)\n",
    "  top_status.plot(kind='barh', color='steelblue')\n",
    "  plt.title('Top 10 códigos de estado HTTP más frecuentes', fontsize=12)\n",
    "  plt.xlabel('Frecuencia')\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url_length'] = df['request_http'].apply(len)\n",
    "df['url_depth'] = df['request_http'].apply(lambda x : x.count('/'))\n",
    "df['n_encoded_chars'] = df['request_http'].apply(lambda x : len(re.findall(r'%[0-9A-Fa-f]{2}', x)))\n",
    "df['n_special_chars'] = df['request_http'].apply(lambda x : len(re.findall(r'[|,;]', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape[0])\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[1, 2, 13]], axis=1)\n",
    "display(df.shape[0])\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='request_url_fragment', axis=1)\n",
    "display(df.shape[0])\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "  'request_url_scheme',\n",
    "  'request_url_netloc',\n",
    "  'request_url_path_str',\n",
    "  'request_url_query_str',\n",
    "  'req_Referer',\n",
    "  'req_User_agent',\n",
    "  'request_method',\n",
    "  'request_protocol'\n",
    "]\n",
    "for col in columns:\n",
    "  print(f\"Valores Únicos de la Columna {col}: {df[col].unique()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433efe0",
   "metadata": {},
   "source": [
    "- Dentro de `request_method` existen algunos métodos como `DELETE`, analizar si estos \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso final: guardar el dataset procesado\n",
    "df.to_csv(log_prep_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
